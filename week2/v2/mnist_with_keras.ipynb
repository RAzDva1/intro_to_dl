{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tf 1.x for colab\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST digits classification with Keras\n",
    "\n",
    "We don't expect you to code anything here because you've already solved it with TensorFlow.\n",
    "\n",
    "But you can appreciate how simpler it is with Keras.\n",
    "\n",
    "We'll be happy if you play around with the architecture though, there're some tips at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnist_sample.png\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(\"We're using TF\", tf.__version__)\n",
    "#import keras\n",
    "#print(\"We are using Keras\", keras.__version__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "#import keras_utils\n",
    "#from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data\n",
    "\n",
    "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import preprocessed_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [shape (50000, 28, 28)] sample patch:\n",
      " [[0.         0.29803922 0.96470588 0.98823529 0.43921569]\n",
      " [0.         0.33333333 0.98823529 0.90196078 0.09803922]\n",
      " [0.         0.33333333 0.98823529 0.8745098  0.        ]\n",
      " [0.         0.33333333 0.98823529 0.56862745 0.        ]\n",
      " [0.         0.3372549  0.99215686 0.88235294 0.        ]]\n",
      "A closeup of a sample patch:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the whole sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [shape (50000,)] 10 samples:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# X contains rgb values divided by 255\n",
    "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print(\"A closeup of a sample patch:\")\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"And the whole sample:\")\n",
    "plt.imshow(X_train[1], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flatten images\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the target\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model with keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# we still need to clear a graph though\n",
    "#s = reset_tf_session()\n",
    "\n",
    "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
    "model.add(Dense(256, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# you can look at all layers and parameter count\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we \"compile\" the model specifying the loss and optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # this is our cross-entropy\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(512, 784), b.shape=(784, 256), m=512, n=256, k=784\n\t [[node dense_1/MatMul (defined at D:\\Program\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_900]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ed7c35bf4e4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_oh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#callbacks=[keras_utils.TqdmProgressCallback()],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(512, 784), b.shape=(784, 256), m=512, n=256, k=784\n\t [[node dense_1/MatMul (defined at D:\\Program\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_900]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# and now we can fit the model with model.fit()\n",
    "# and we don't have to write loops and batching manually as in TensorFlow\n",
    "model.fit(\n",
    "    X_train_flat, \n",
    "    y_train_oh,\n",
    "    batch_size=512, \n",
    "    epochs=40,\n",
    "    validation_data=(X_val_flat, y_val_oh),\n",
    "    #callbacks=[keras_utils.TqdmProgressCallback()],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here're the notes for those who want to play around here\n",
    "\n",
    "Here are some tips on what you could do:\n",
    "\n",
    " * __Network size__\n",
    "   * More neurons, \n",
    "   * More layers, ([docs](https://keras.io/))\n",
    "\n",
    "   * Other nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "\n",
    " * __Early Stopping__\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "     \n",
    "\n",
    " * __Faster optimization__\n",
    "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "\n",
    "\n",
    " * __Regularize__ to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "     * Can be done manually or via - https://keras.io/regularizers/\n",
    "   \n",
    "   \n",
    " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
    "   * https://keras.io/preprocessing/image/\n",
    "   * Zoom-in+slice = move\n",
    "   * Rotate+zoom(to remove black stripes)\n",
    "   * any other perturbations\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
